# AI Document RAG Service ğŸš€  
### LangChain + FastAPI + OpenAI

A simple **Retrieval-Augmented Generation (RAG)** API built using **LangChain**, **OpenAI**, and **FastAPI**.

This project demonstrates how to:

- Load and process documents  
- Split documents into chunks  
- Generate embeddings  
- Build a vector store  
- Retrieve relevant context  
- Generate answers using an LLM  
- Expose everything through a FastAPI endpoint  

---

## ğŸ“Œ Features

- ğŸ“„ Document loading from `data/`
- âœ‚ï¸ Chunking with `RecursiveCharacterTextSplitter`
- ğŸ”¢ OpenAI Embeddings
- ğŸ—„ Vector Store (FAISS / Chroma)
- ğŸ” Similarity-based retrieval (Top-K)
- âš¡ FastAPI REST endpoint
- ğŸ“š Swagger UI documentation

---

```
AI-Document-RAG-Service/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ my_document.txt
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ image.png
â”‚
â”œâ”€â”€ rag.py
â”œâ”€â”€ endpoints.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ .env (NOT committed)
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/AI-Document-RAG-Service.git
cd AI-Document-RAG-Service
```

---

### 2ï¸âƒ£ Create a virtual environment (optional but recommended)

```bash
python -m venv venv
venv\Scripts\activate   # Windows
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Create `.env` file

Create a `.env` file in the root directory:

```env
OPENAI_API_KEY=your_openai_api_key_here
```

âš ï¸ Do **NOT** commit `.env` to GitHub.

---

## â–¶ï¸ Run the API

```bash
uvicorn main:app --reload
```

Open in your browser:

```
http://127.0.0.1:8000/docs
```

Swagger UI will appear.

---

## ğŸ” Query Example

### Endpoint

```
GET /query/?query=Are polar bears in danger?
```

### Example curl request

```bash
curl "http://127.0.0.1:8000/query/?query=Are%20polar%20bears%20in%20danger%3F"
```

---

## ğŸ–¼ Demo Screenshot

![Swagger Demo](assets/image.png)

---

## ğŸ§  How It Works

1. Document is loaded from `data/my_document.txt`
2. Text is split into chunks
3. Embeddings are generated using OpenAI
4. A vector store is created
5. Relevant chunks are retrieved for a query
6. LLM generates the final answer using retrieved context

---

## âš ï¸ Windows OpenMP Fix (If Needed)

If you encounter this error:

```
OMP: Error #15: libiomp5md.dll already initialized
```

Run the server with:

```powershell
$env:KMP_DUPLICATE_LIB_OK="TRUE"
uvicorn main:app --reload
```

---

## ğŸ›¡ Security Note

- Never commit your `.env` file.
- Always rotate your API key if accidentally exposed.
- Add `.env` to `.gitignore`.

---
